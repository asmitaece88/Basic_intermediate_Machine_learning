{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will be using the NLTK library for removing stopwords. NLTK comes with several stopword corpora, \n",
    "we will be using the English corpus. \n",
    "This corpus contains a huge number of English stopwords like a, the, be, for, do, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords=stopwords.words('english')\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have a list of stopwords. When we process our text data we will iterate through each word, if it is present in stop_words it will be removed. To optimize the speed of the stopword lookup we can convert stop_words to a set object.\n",
    "\n",
    "# first we need  to lowercase  our text . then we use our input  text into list of tokens( each token separated is a word   separated by space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i’m',\n",
       " 'amazed',\n",
       " 'how',\n",
       " 'often',\n",
       " 'in',\n",
       " 'practice,',\n",
       " 'not',\n",
       " 'only',\n",
       " 'does',\n",
       " 'a',\n",
       " '@huggingface',\n",
       " 'nlp',\n",
       " 'model',\n",
       " 'solve',\n",
       " 'your',\n",
       " 'problem,',\n",
       " 'but',\n",
       " 'one',\n",
       " 'of',\n",
       " 'their',\n",
       " 'public',\n",
       " 'finetuned',\n",
       " 'checkpoints,',\n",
       " 'is',\n",
       " 'good',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'the',\n",
       " 'job.',\n",
       " 'both',\n",
       " 'impressed,',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'disappointed',\n",
       " 'how',\n",
       " 'rarely',\n",
       " 'i',\n",
       " 'get',\n",
       " 'to',\n",
       " 'actually',\n",
       " 'train',\n",
       " 'a',\n",
       " 'model',\n",
       " 'that',\n",
       " 'matters',\n",
       " ':(']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=tweet.lower().split()\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have a list of stopwords. When we process our text data we will iterate through each word, if it is present in stop_words it will be removed. To optimize the speed of the stopword lookup we can convert stop_words to a set object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with stopwords\n",
      "  i’mamazedhowofteninpractice,notonlydoesa@huggingfacenlpmodelsolveyourproblem,butoneoftheirpublicfinetunedcheckpoints,isgoodenoughforthejob.bothimpressed,andalittledisappointedhowrarelyigettoactuallytrainamodelthatmatters:(\n",
      "without \n",
      " : i’mamazedoftenpractice,@huggingfacenlpmodelsolveproblem,onepublicfinetunedcheckpoints,goodenoughjob.impressed,littledisappointedrarelygetactuallytrainmodelmatters:(\n"
     ]
    }
   ],
   "source": [
    "tweet_no_stopwords=[word for word in tweet if word not in stop_words]\n",
    "print(\"with stopwords\\n \",''. join(tweet))\n",
    "print(\"without \\n :\",''.join(tweet_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We breifly mentioned the word token in the last section, where I described it as a word. In some cases this is true, in others not. A token is a single unit, or piece of information. Typically in NLP we will find that models consume a token, which can represent a multitude of different things, such as:\n",
    "\n",
    "A word\n",
    "Part of a word\n",
    "A single character\n",
    "Puntuation mark *[,!-.]*\n",
    "Special token like , or\n",
    "Model-specific special tokens, like *[CLS]* and *[SEP]* for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’m',\n",
       " 'amazed',\n",
       " 'how',\n",
       " 'often',\n",
       " 'in',\n",
       " 'practice,',\n",
       " 'not',\n",
       " 'only',\n",
       " 'does',\n",
       " 'a',\n",
       " '@huggingface',\n",
       " 'NLP',\n",
       " 'model',\n",
       " 'solve',\n",
       " 'your',\n",
       " 'problem,',\n",
       " 'but',\n",
       " 'one',\n",
       " 'of',\n",
       " 'their',\n",
       " 'public',\n",
       " 'finetuned',\n",
       " 'checkpoints,',\n",
       " 'is',\n",
       " 'good',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'the',\n",
       " 'job.',\n",
       " 'Both',\n",
       " 'impressed,',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'disappointed',\n",
       " 'how',\n",
       " 'rarely',\n",
       " 'I',\n",
       " 'get',\n",
       " 'to',\n",
       " 'actually',\n",
       " 'train',\n",
       " 'a',\n",
       " 'model',\n",
       " 'that',\n",
       " 'matters',\n",
       " ':(']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\"\n",
    "\n",
    "tweet.split()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is the rawest form of word-level tokens, the alternative to word-level is character-level, which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " '’',\n",
       " 'm',\n",
       " ' ',\n",
       " 'a',\n",
       " 'm',\n",
       " 'a',\n",
       " 'z',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'a',\n",
       " 'c',\n",
       " 't',\n",
       " 'i']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[char for char  in tweet][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# advantage of character leve tokens over word  tokens "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The 'advantage' of using character-level embeddings is that \n",
    "any models we train on this data will only need to remember all of the characters of the alphabet, \n",
    "punctuation characters, and spaces/newlines. So the model vocabulary (list of all the tokens it knows) is very small.\n",
    "Additionally if a new word appears outside of training, the model will still be able to digest it - \n",
    "whereas a word-level embedding model would not understand the new word and replace it with an unknown token \n",
    "(more on this soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# disadvantage of charcater lvel tkens over word tokens \n",
    "# It's not all good news for character-level embeddings though. Words carry a significant level of semantic meaning,\n",
    "# and when we use character-level embedding this is mostly lost. \n",
    "# At a high-level we can view character-level embedding as being good for syntax, \n",
    "# and word-level embedding as being better for semantics. Although, in-reality, word-level embeddings \n",
    "# almost always outpeform character-level embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to word-level embeddings, we will often find with the latest transformer models that text can be split into part-word tokens. So for example, we may find that the word 'being' is split into the tokens [\"be\", \"-ing\"], or 'amazingly' to [\"amaz\", \"-ing\", \"-ly\"].\n",
    "\n",
    "# In addition to this, we typically seperate punctuation too, so in our previous example the tokens '@huggingface' and 'impressed,' would become [\"@\", \"huggingface\"] and [\"impressed\", \",\"] respectively.\n",
    "\n",
    "# In our tweet we might want to find any token that begins with @ and convert that token to , a unique token that we have specified to identify usernames in our tweets. This rule is logical as there are potentially millions of added tokens in our model if we include Twitter usernames, but the username doesn't tell our model anything about the meaning in the language of the text, for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @elonmusk thinks that the NLP models that @joebloggs made are super cool\n",
    "\n",
    "# Has no real meaningful difference to our model as with:\n",
    "\n",
    "# @joebloggs thinks that the NLP models that @huggingface made are super cool\n",
    "\n",
    "# The meaning and subsequent classification of both tweets should really be identical in our model. So, it is logical to replace usernames with a single shared token. This approach is something that is commonly used for many different things such as:\n",
    "\n",
    "# emails\n",
    "# names/usernames\n",
    "# URLs\n",
    "# monetary values\n",
    "# or any other numbers\n",
    "# But ofcourse we don't always want to do this for everything, this is simply a rough guide as to what we may want to tokenize."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally, we also need to understand model-specific special tokens. We will do this with an example.\n",
    "\n",
    "For the BERT transformer model there are five special tokens that are used by the model, these are:\n",
    "\n",
    "Token\tMeaning\n",
    "[PAD]\tPadding token, allows us to maintain same-length sequences (512 tokens for Bert) even when different sized sentences are fed in\n",
    "[UNK]\tUsed when a word is unknown to Bert\n",
    "[CLS]\tAppears at the start of every sequence\n",
    "[SEP]\tIndicates a seperator or end of sequence\n",
    "[MASK]\tUsed when masking tokens, for example in training with masked language modelling (MLM)\n",
    "So if we take the 'NLP models' tweet, processing that directly with our BERT specific tokens might look like this:\n",
    "\n",
    "['[CLS]', '[UNK]', 'thinks', 'that', 'the', 'nlp', 'models', 'that', '[UNK]', 'made', 'are', 'super', 'cool', '[SEP]', '[PAD]', '[PAD]', ..., '[PAD]']\n",
    "Here, we have:\n",
    "\n",
    "Applied [CLS] token to indicate the start of the sequence.\n",
    "Both username tokens @elonmusk and @joebloggs were not 'known' words to BERT so BERT replaced them with unknown tokens [UNK], alternatively we could have replaced these with our own special user tokens.\n",
    "Added *[SEP] token to the end of our sequence.\n",
    "Padded the sequence upto the required length of 512 tokens (required due to fixed input sequence length of BERT model) using [PAD] tokens.\n",
    "Different models will have different special tokens, but we will often that they are being used for similiar reasons.\n",
    "\n",
    "That's everything on tokens for now, although we will cover tokenization in more depth (and the code too) for different models in later notebooks.\n",
    "\n",
    "1\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stemming is a text normalization method used in NLP to simplify text before it is processed by a model. When stemming break the final few characters of a word in order to find a common form of the word. If we take the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"I am amazed by how amazingly amazing you are\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We use different forms of the word amaze a total of three times. Each of these different forms is called an 'inflection', which is the modification of a word to slightly adjust the meaning or context of the word. When we tokenize this text we produce three different tokens for each inflection of happy, which is okay but in many applications this level of granularity in the semantic meaning of the word is not required and can damage model performance.\n",
    "\n",
    "Later, when we get to using more complex, sophisticated models (eg BERT), we will use different methods that maintain the inflection of each word - but it is important to understand stemming as it was a very important part of text preprocessing for a very long time, and still relevant to many applications.\n",
    "\n",
    "To apply stemming we will be using the NLTK package, which provides several different stemmers, we will test the PorterStemmer and LancasterStemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between porterstemmer vs LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = ['happy', 'happiest', 'happier', 'cactus', 'cactii', 'elephant', 'elephants', 'amazed', 'amazing', 'amazingly', 'cement', 'owed', 'maximum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmed = [porter.stem(word) for word in words_to_stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_stemmed=[lancaster.stem(word) for word in words_to_stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happi',\n",
       " 'happiest',\n",
       " 'happier',\n",
       " 'cactu',\n",
       " 'cactii',\n",
       " 'eleph',\n",
       " 'eleph',\n",
       " 'amaz',\n",
       " 'amaz',\n",
       " 'amazingli',\n",
       " 'cement',\n",
       " 'owe',\n",
       " 'maximum']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy',\n",
       " 'happiest',\n",
       " 'happy',\n",
       " 'cact',\n",
       " 'cacti',\n",
       " 'eleph',\n",
       " 'eleph',\n",
       " 'amaz',\n",
       " 'amaz',\n",
       " 'amaz',\n",
       " 'cem',\n",
       " 'ow',\n",
       " 'maxim']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Porter stemmer is a set of rules that strip common suffixes from the ends of words, each of these rules are applied on after the other and produce our Porter stemmed words. It is a simple stemmer, and very fast.\n",
    "\n",
    "The Lancaster stemmer contains a larger set of rules and rather than applying each rule one after the other will keep iterating through the list of rules and find a rule that matches the current condition, which will then delete or replace the ending of the word. The iterations will stop once no more rules can be applied to the word OR if the word starts with a vowel and only two characters remain OR if the word starts with a consonant and there are three characters remaining. The Lancaster stemmer is much more aggressive in its stemming, sometimes this is a good thing, sometimes not.\n",
    "\n",
    "We can see from the results of the two stemmers above that neither are perfect, and this is the case with all stemming algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization¶"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lemmatization is very similiar to stemming in that it reduces a set of inflected words down to a common word. The difference is that lemmatization reduces inflections down to their real root words, which is called a lemma. If we take the words 'amaze', 'amazing', 'amazingly', the lemma of all of these is 'amaze'. Compared to stemming which would usually return 'amaz'. Generally lemmatization is seen as more advanced than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['amaze', 'amazed', 'amazing']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will use NLTK again for our lemmatization. We also need to ensure we have the WordNet Database downloaded which will act as the lookup for our lemmatizer to ensure that it has produced a real lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Asmita\n",
      "[nltk_data]     Chatterjee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['amaze', 'amazed', 'amazing']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "[lemmatizer.lemmatize(word) for word in  words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearly nothing has happened, and that is because lemmatization requires that we also provide the parts-of-speech (POS) tag, which is the category of a word based on syntax. For example noun, adjective, or verb. In our case we could place each word as a verb, which we can then implement like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amaze', 'amaze', 'amaze']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "    \n",
    "    \n",
    "[lemmatizer.lemmatize(word,wordnet.VERB) for word in  words]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
